llm_model_name: "gemma-2-9b-it"
seed: 42

training_params:
  learning_rate: 0.001
  epochs: 100


#dataset
pca:
  use_pca: False
  n_components: 100
  layer_wise: False

use_coveig: False

nn:
  num_layers: 3

lstm:
  hidden_size: 50
  num_layers: 3
