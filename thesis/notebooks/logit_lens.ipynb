{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_pickle(filename,path):\n",
    "    with open(path+filename + \".pkl\", \"rb\") as file:\n",
    "        my_list = pickle.load(file)\n",
    "    return my_list\n",
    "        \n",
    "\n",
    "original_dir  = \"/home/knowledgeconflict/home/martin/MasterThesis/data/logit_lens/original/\"\n",
    "transformed_dir = \"/home/knowledgeconflict/home/martin/MasterThesis/data/logit_lens/fake/\"\n",
    "\n",
    "#load original lists \n",
    "\n",
    "original_final_answers = load_pickle(filename=\"original_answers\",path=original_dir)\n",
    "original_labeled_true_scores = torch.load(original_dir+\"labeled_true_scores\"+\".pt\", map_location=torch.device('cpu'))\n",
    "original_labeled_false_scores = torch.load(original_dir+\"labeled_false_scores\"+\".pt\", map_location=torch.device('cpu'))\n",
    "original_topk_tokens = load_pickle(filename=\"topk_tokens\",path=original_dir)\n",
    "original_topk_scores = torch.load(original_dir+\"topk_scores\"+\".pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "#load fake lists \n",
    "\n",
    "transformed_final_answers = load_pickle(filename=\"original_answers\",path=transformed_dir)\n",
    "transformed_labeled_true_scores = torch.load(transformed_dir+\"labeled_true_scores\"+\".pt\", map_location=torch.device('cpu'))\n",
    "transformed_labeled_false_scores = torch.load(transformed_dir+\"labeled_false_scores\"+\".pt\", map_location=torch.device('cpu'))\n",
    "transformed_topk_tokens = load_pickle(filename=\"topk_tokens\",path=transformed_dir)\n",
    "transformed_topk_scores = torch.load(transformed_dir+\"topk_scores\"+\".pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_final_answers)\n",
    "original_hallucinated_indices =  [i for i, x in enumerate(original_final_answers) if x == \"FALSE\"]\n",
    "print(transformed_final_answers)\n",
    "transformed_hallucinated_indices =[i for i, x in enumerate(transformed_final_answers) if x == \"TRUE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_inputs = (len(original_final_answers)+len(transformed_final_answers))\n",
    "failures = len(original_hallucinated_indices)+len(transformed_hallucinated_indices)\n",
    "print(f\"Accuracy {(length_inputs-failures)/length_inputs*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print layer token development\n",
    "print(\"For one sample only\")\n",
    "for o_topk,t_topk in zip (original_topk_tokens[:1],transformed_topk_tokens[:1]):\n",
    "        print(o_topk)\n",
    "        print(t_topk)\n",
    "        \n",
    "for o_topk,t_topk in zip (original_topk_scores[:1],transformed_topk_scores[:1]):\n",
    "        print(o_topk[:,:,:1].tolist())\n",
    "        print(t_topk[:,:,:1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true vs false development \n",
    "print(\"For one sample only\")\n",
    "print(\"TRUE\")\n",
    "for o_topk,t_topk in zip (original_labeled_true_scores[:1],transformed_labeled_true_scores[:1]):\n",
    "        print(o_topk.tolist())\n",
    "        print(t_topk.tolist())\n",
    "print(\"FALSE\")\n",
    "for o_topk,t_topk in zip (original_labeled_false_scores[:1],transformed_labeled_false_scores[:1]):\n",
    "        print(o_topk.tolist())\n",
    "        print(t_topk.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dif = original_labeled_true_scores-original_labeled_false_scores\n",
    "print(original_dif[0].tolist())\n",
    "\n",
    "transformed_dif = transformed_labeled_true_scores-transformed_labeled_false_scores\n",
    "print(transformed_dif[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_avg_original_dif = torch.mean(original_dif,dim=0)\n",
    "layer_avg_transformed_dif = torch.mean(transformed_dif,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(tensor, title=\"Tensor Plot\"):\n",
    "    # Check if tensor is from PyTorch or TensorFlow and convert it to NumPy array\n",
    "    data = tensor.detach().numpy()  # For PyTorch\n",
    "\n",
    "    # Plot the data\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def four_subplot(original_true_tensor, original_false_tensor, transformed_true_tensor, transformed_false_tensor):\n",
    "    # Calculate the maximum value across all tensors\n",
    "    max_value = max(\n",
    "        max(original_true_tensor), \n",
    "        max(original_false_tensor), \n",
    "        max(transformed_true_tensor), \n",
    "        max(transformed_false_tensor)\n",
    "    )\n",
    "    \n",
    "    # Create a 2x2 subplot grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(5, 5))\n",
    "    \n",
    "    # Plot each tensor on the respective subplot\n",
    "    axs[0, 0].plot(original_true_tensor, color='blue')\n",
    "    axs[0, 0].set_title('Original True Tensor')\n",
    "    axs[0, 0].set_ylim(0, max_value)  # Set y-axis limit\n",
    "    axs[0, 0].grid(True) \n",
    "\n",
    "    axs[0, 1].plot(original_false_tensor, color='orange')\n",
    "    axs[0, 1].set_title('Original False Tensor')\n",
    "    axs[0, 1].set_ylim(0, max_value)  # Set y-axis limit\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    axs[1, 0].plot(transformed_true_tensor, color='green')\n",
    "    axs[1, 0].set_title('Transformed True Tensor')\n",
    "    axs[1, 0].set_ylim(0, max_value)  # Set y-axis limit\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    axs[1, 1].plot(transformed_false_tensor, color='red')\n",
    "    axs[1, 1].set_title('Transformed False Tensor')\n",
    "    axs[1, 1].set_ylim(0, max_value)  # Set y-axis limit\n",
    "    axs[1, 1].grid(True)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_tensors(original_tensor, transformed_tensor, original_label=\"Original Difference\", transformed_label=\"Transformed Difference\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    max_value = max(max(original_tensor),max(transformed_tensor))\n",
    "    min_value = min(min(original_tensor),min(transformed_tensor))\n",
    "    \n",
    "    # Plot the original tensor\n",
    "    plt.plot(original_tensor, label=original_label, color='blue')\n",
    "    \n",
    "    # Plot the transformed tensor\n",
    "    plt.plot(transformed_tensor, label=transformed_label, color='orange')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(\"Comparison of Original and Transformed Differences\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.ylim(min_value,max_value)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show grid for better readability\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_tensors(layer_avg_original_dif,layer_avg_transformed_dif)\n",
    "print(list(zip(layer_avg_original_dif,layer_avg_transformed_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_tensors(layer_avg_original_dif,layer_avg_transformed_dif)\n",
    "print(list(zip(layer_avg_original_dif,layer_avg_transformed_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_hallucinated_mean = torch.mean(original_dif[original_hallucinated_indices, :], dim=0)\n",
    "transformed_hallucinated_mean = torch.mean(transformed_dif[transformed_hallucinated_indices, :], dim=0)\n",
    "\n",
    "original_other_indices = list(set(range(len(original_dif))) - set(original_hallucinated_indices))\n",
    "transformed_other_indices = list(set(range(len(transformed_dif))) - set(transformed_hallucinated_indices))\n",
    "\n",
    "original_non_hallucinated_mean = torch.mean(original_dif[original_other_indices, :], dim=0)\n",
    "transformed_non_hallucinated_mean = torch.mean(transformed_dif[transformed_other_indices, :], dim=0)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each line\n",
    "x = range(original_hallucinated_mean.shape[0])\n",
    "plt.plot(x, original_hallucinated_mean.numpy(), label='Original Hallucinated Mean', color='blue', linestyle='--', linewidth=2)\n",
    "plt.plot(x, transformed_hallucinated_mean.numpy(), label='Transformed Hallucinated Mean', color='orange', linestyle='--', linewidth=2)\n",
    "plt.plot(x, original_non_hallucinated_mean.numpy(), label='Original Non-Hallucinated Mean', color='green', linestyle='-', linewidth=2)\n",
    "plt.plot(x, transformed_non_hallucinated_mean.numpy(), label='Transformed Non-Hallucinated Mean', color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Difference of True and False Logits')\n",
    "plt.title('Comparison of Logit Development for Hallucinated and Non-Hallucinated')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_subplot(original_labeled_true_scores.mean(dim=0),original_labeled_false_scores.mean(dim=0),transformed_labeled_true_scores.mean(dim=0),transformed_labeled_false_scores.mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_subplot(original_labeled_true_scores[original_hallucinated_indices,:].mean(dim=0),original_labeled_false_scores[original_hallucinated_indices,:].mean(dim=0),transformed_labeled_true_scores[transformed_hallucinated_indices,:].mean(dim=0),transformed_labeled_false_scores[transformed_hallucinated_indices,:].mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Early Exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, labels):\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds)\n",
    "    recall = recall_score(y_true=labels, y_pred=preds)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "original_hallucinated = original_dif[original_hallucinated_indices, :]\n",
    "transformed_hallucinated = transformed_dif[transformed_hallucinated_indices, :]\n",
    "\n",
    "original_other_indices = list(set(range(len(original_dif))) - set(original_hallucinated_indices))\n",
    "transformed_other_indices = list(set(range(len(transformed_dif))) - set(transformed_hallucinated_indices))\n",
    "\n",
    "original_non_hallucinated = original_dif[original_other_indices, :]\n",
    "transformed_non_hallucinated = transformed_dif[transformed_other_indices, :]\n",
    "\n",
    "inputs = torch.cat([original_hallucinated,transformed_hallucinated,original_non_hallucinated,transformed_non_hallucinated])\n",
    "num_of_hallucinated = len(original_hallucinated)+len(transformed_hallucinated)\n",
    "num_of_non_hallucinated = len(original_non_hallucinated)+len(transformed_non_hallucinated)\n",
    "labels = torch.cat([torch.Tensor([1]*num_of_hallucinated),torch.Tensor([0]*num_of_non_hallucinated)]).unsqueeze(1)\n",
    "\n",
    "inputs = inputs.cpu().numpy()\n",
    "labels = labels.cpu().numpy()\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train log reg\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "num_layers = X_train.shape[-2]\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "acc, prec, rec, f1 =  calculate_metrics(log_reg.predict(X_train),y_train)\n",
    "print({\"train_acc\": acc, \"train_precision\": prec, \"train_recall\": rec, \"train_f1\": f1})\n",
    "\n",
    "acc, prec, rec, f1 =  calculate_metrics(log_reg.predict(X_test),y_test)\n",
    "print({\"val_acc\": acc, \"val_precision\": prec, \"val_recall\": rec, \"f1\": f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "grad_boost = GradientBoostingClassifier(n_estimators=150)\n",
    "num_layers = X_train.shape[-2]\n",
    "\n",
    "grad_boost.fit(X_train, y_train)\n",
    "\n",
    "acc, prec, rec, f1 =  calculate_metrics(grad_boost.predict(X_train),y_train)\n",
    "print({\"train_acc\": acc, \"train_precision\": prec, \"train_recall\": rec, \"train_f1\": f1})\n",
    "\n",
    "acc, prec, rec, f1 =  calculate_metrics(grad_boost.predict(X_test),y_test)\n",
    "print({\"val_acc\": acc, \"val_precision\": prec, \"val_recall\": rec, \"f1\": f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis-5ARcKmzf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
